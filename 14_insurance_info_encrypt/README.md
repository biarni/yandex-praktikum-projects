# Защита персональных данных клиентов

Вам нужно защитить данные клиентов страховой компании «Хоть потоп». Разработайте такой метод преобразования данных, чтобы по ним было сложно восстановить персональную информацию. Обоснуйте корректность его работы.

Нужно защитить данные, чтобы при преобразовании качество моделей машинного обучения не ухудшилось. Подбирать наилучшую модель не требуется.

## Умножение матриц

Ответим на вопрос.  
Признаки умножают на некую обратимую матрицу. Изменится ли качество линейной регрессии? (Её можно обучить заново). Варианты ответов:  
a. Изменится.   
b. Не изменится. Укажем в этом случае, как связаны параметры линейной регрессии в исходной задаче и в преобразованной.  

Обозначения:

- $X$ — матрица признаков (нулевой столбец состоит из единиц)

- $y$ — вектор целевого признака

- $P$ — обратимая матрица, на которую умножаются признаки

- $w$ — вектор весов линейной регрессии (нулевой элемент равен сдвигу)

**Для исходной матрицы с признаками:**  

Предсказания:

$$
a = Xw
$$

Задача обучения:

$$
w = \arg\min_w MSE(Xw, y)
$$

Формула обучения:

$$
w = (X^T X)^{-1} X^T y
$$

**Для исходной матрицы с признаками, умноженной на случайную обратимую матрицу:**  

Предсказания:

$$
a' = XPw'
$$

Задача обучения:

$$
w' = \arg\min_w' MSE(XPw', y)
$$

Формула обучения:

$$
w' = ((XP)^T XP)^{-1} (XP)^T y
$$

**Преобразуем формулу нахождения вектора весов:**

$w' = ((XP)^TXP)^{-1} (XP)^Ty = P^{-1}((XP)^{T}X)^{-1} (XP)^Ty = P^{-1}((XP)^{T}X)^{-1}P^TX^Ty =
P^{-1}(P^TX^TX)^{-1} P^TX^Ty = P^{-1}(X^TX)^{-1}(P^T)^{-1} P^TX^Ty$

Сократим: $(P^{T})^{-1} P^T$ 

Получается: $w'= P^{-1}(X^TX)^{-1} X^Ty$

Тогда:
$a' = XPw' = XPP^{-1}(X^TX)^{-1} X^Ty$  

Сократим: $PP^{-1}$   

Получается: $a' = X(X^TX)^{-1} X^Ty = Xw = a$

**Вывод:** При умножении матрицы признаков на случайную обратимую матрицу предсказания модели линейной регрессии не изменятся.

## Алгоритм преобразования

Предложим алгоритм преобразования данных для решения задачи. Обоснуем, почему качество линейной регрессии не поменяется.

**Алгоритм:**  
1. Целевой признак вынесем из набора данных.
2. Сгенерируем случайную квадратную матрицу.
3. Проверим ее обратимость.
4. Умножим матрицу с признаками на ключ шифрования.
5. Расшифровать матрицу признаков можно путем ее умножения на обратную матрицу ключа шифрования.

**Обоснование:**  

Как показано в разделе 2 - для линейной регрессии умножение признаков на случайную обратимую матрицу не влияет на предсказания модели. Вместе с тем, при матричном умножении по двум матрицам строится третья, которая состоит из скалярных произведений строк первой матрицы на столбцы второй. Таким образом, из преобразованных данных без знания ключа шифрования невозможно будет восстановить первоначальные.

Для обратного преобразования (расшифровки) верно следцющее. Если любую матрицу A умножить на единичную матрицу E (или наоборот), получится эта же матрица A. При этом, умножение матрицы на обратную ей матрицу дает единичную матрицу.   
Таким образом: $XPP^{-1} = XE = X$

## Вывод

Качество линейной регрессии из sklearn не отличается до и после преобразования. Шифрование путем умножения признаков на случайную обратимую матрицу и обратное дешифрование проходят успешно. Алгоритм проверен, рабочий. Все удалось. Ура, товарищи, клиенты могут спать спокойно - их данные защищены!